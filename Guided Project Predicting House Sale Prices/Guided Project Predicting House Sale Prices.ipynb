{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d551d76",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this course, we started by building intuition for model based learning, explored how the linear regression model worked, understood how the two different approaches to model fitting worked, and some techniques for cleaning, transforming, and selecting features. In this guided project, you can practice what you learned in this course by exploring ways to improve the models we built.\n",
    "\n",
    "You'll work with housing data for the city of Ames, Iowa, United States from 2006 to 2010. You can read more about why the data was collected here. You can also read about the different columns in the data here.\n",
    "\n",
    "Let's start by setting up a pipeline of functions that will let us quickly iterate on different models.\n",
    "\n",
    "Train----> transform featuress()----> select features ()------> train_and_test()-----> rmse,avg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80da973",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "* Import pandas, matplotlib, and numpy into the environment. Import the classes you need from scikit-learn as well.\n",
    "* Read `AmesHousing.tsv` into a pandas data frame.\n",
    "* For the following functions, we recommend creating them in the first few cells in the notebook. This way, you can add cells to the end of the notebook to do experiments and update the functions in these cells.\n",
    "  * Create a function named `transform_features()` that, for now, just returns the `train` data frame.\n",
    "  * Create a function named `select_features()` that, for now, just returns the `Gr Liv Area` and `SalePrice` columns from the train data frame.\n",
    "  * Create a function named `train_and_test()` that, for now:\n",
    "    * Selects the first 1460 rows from from data and assign to `train`.\n",
    "    * Selects the remaining rows from data and assign to `test`.\n",
    "    * Trains a model using all numerical columns except the `SalePrice column` (the target column) from the data frame returned from `select_features()`\n",
    "    * Tests the model on the test set and returns the `RMSE` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db737bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pandas, matplotlib, and numpy into the environment.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Read AmesHousing.tsv into a pandas data frame.\n",
    "df=pd.read_csv('AmesHousing.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf7a6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57088.25161263909"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a function named transform_features() that, for now, just returns the train data frame.\n",
    "def transform_features(df):\n",
    "    return df\n",
    "\n",
    "#Create a function named select_features() that, for now, just returns the Gr Liv Area and \n",
    "#SalePrice columns from the train data frame.\n",
    "def select_features(df):\n",
    "    return df[['Gr Liv Area,','SalePrice']]\n",
    "def select_features(df):\n",
    "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
    "\n",
    "\n",
    "#Create a function named train_and_test()\n",
    "def train_and_test(df):\n",
    "    train_df=df[:1460]\n",
    "    test_df=df[1460:]\n",
    "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
    "    ## and return only those columns as a data frame.\n",
    "    numeric_train = train_df.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test_df.select_dtypes(include=['integer', 'float'])\n",
    "    features=numeric_train.columns.drop(['SalePrice'])\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train_df[features], train_df[\"SalePrice\"])\n",
    "    predictions = lr.predict(test_df[features])\n",
    "    mse = mean_squared_error(test_df[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb376c2f",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Let's now start removing features with many missing values, diving deeper into potential categorical features, and transforming text and numerical columns. Update `transform_features()` so that any column from the data frame with more than 25% (or another cutoff value) missing values is dropped. You also need to remove any columns that leak information about the sale (e.g. like the year the sale happened). In general, the goal of this function is to:\n",
    "\n",
    "* remove features that we don't want to use in the model, just based on the number of missing values or data leakage\n",
    "* transform features into the proper format (numerical to categorical, scaling numerical, filling in missing values, etc)\n",
    "* create new features by combining other features\n",
    "\n",
    "Next, you need to get more familiar with the remaining columns by reading the data documentation for each column, determining what transformations are necessary (if any), and more. As we mentioned earlier, succeeding in predictive modeling (and competitions like Kaggle) is highly dependent on the quality of features the model has. Libraries like scikit-learn have made it quick and easy to simply try and tweak many different models, but cleaning, selecting, and transforming features are still more of an art that requires a bit of human ingenuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6584a6",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "* As we mentioned earlier, we recommend adding some cells to explore and experiment with different features (before rewriting these functions).\n",
    "* The `transform_features()` function shouldn't modify the train data frame and instead return a new one entirely. This way, we can keep using train in the experimentation cells.\n",
    "* Which columns contain less than 5% missing values?\n",
    "  * For numerical columns that meet this criteria, let's fill in the missing values using the most popular value for that column.\n",
    "* What new features can we create, that better capture the information in some of the features?\n",
    "  * An example of this would be the `years_until_remod` feature we created in the last lesson.\n",
    "* Which columns need to be dropped for other reasons?\n",
    "* Which columns aren't useful for machine learning?\n",
    "* Which columns leak data about the final sale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8973d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_df=df.isnull().sum() * 100/len(df)\n",
    "#drop columns with missing values greater than 5%\n",
    "missing_val_greater_5=percent_df[(percent_df>5)]\n",
    "df=df.drop(missing_val_greater_5.index,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d657f8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mas Vnr Area': 0.0,\n",
       " 'BsmtFin SF 1': 0.0,\n",
       " 'BsmtFin SF 2': 0.0,\n",
       " 'Bsmt Unf SF': 0.0,\n",
       " 'Total Bsmt SF': 0.0,\n",
       " 'Bsmt Full Bath': 0.0,\n",
       " 'Bsmt Half Bath': 0.0,\n",
       " 'Garage Cars': 2.0,\n",
       " 'Garage Area': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select the missing values less than 5%\n",
    "missing_val_less_5=df[percent_df[(percent_df>0) & (percent_df<5)].index]\n",
    "\n",
    "#For numerical columns that meet this criteria, let's fill in the missing \n",
    "#values using the most popular value for that column.\n",
    "numeric_df=missing_val_less_5.select_dtypes(include=['integer', 'float'])\n",
    "### Compute the most common value for each column in `fixable_nmeric_missing_cols`.\n",
    "replacement_values_dict = numeric_df.mode().to_dict(orient='records')[0]\n",
    "replacement_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609908d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Use `pd.DataFrame.fillna()` to replace missing values.\n",
    "df = df.fillna(replacement_values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e76f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop object columns with one or more missing values\n",
    "\n",
    "text_cols = df.select_dtypes(include=['object'])\n",
    "text_cols_count=text_cols.isnull().sum()\n",
    "## Filter Series to columns containing *any* missing values\n",
    "missing_text_cols = text_cols_count[text_cols_count > 0]\n",
    "\n",
    "df = df.drop(missing_text_cols.index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c57f384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order             0\n",
       "PID               0\n",
       "MS SubClass       0\n",
       "MS Zoning         0\n",
       "Lot Area          0\n",
       "                 ..\n",
       "Mo Sold           0\n",
       "Yr Sold           0\n",
       "Sale Type         0\n",
       "Sale Condition    0\n",
       "SalePrice         0\n",
       "Length: 64, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify that every column has 0 missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073df2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create new feature columns\n",
    "\n",
    "df['years_until_remod']= df['Year Remod/Add'] - df['Year Built']\n",
    "\n",
    "df['years_since_remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "\n",
    "df['years_before_sale']= df['Yr Sold'] - df['Year Built']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05293c72",
   "metadata": {},
   "source": [
    "let's check and see if there are negative values in our new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036048bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850   -1\n",
       "Name: years_until_remod, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['years_until_remod'][df['years_until_remod'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5944ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702   -1\n",
       "2180   -2\n",
       "2181   -1\n",
       "Name: years_since_remod, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['years_since_remod'][df['years_since_remod'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c43a366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180   -1\n",
       "Name: years_before_sale, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['years_before_sale'][df['years_before_sale'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fb355",
   "metadata": {},
   "source": [
    "apparently, there are negative values in our dataset which i wasn't expecting, but let's dive deeeper to find out reasons for this.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4bb315f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "years_since_remod      -2\n",
       "Yr Sold              2007\n",
       "Year Remod/Add       2009\n",
       "Name: 2180, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['years_since_remod','Yr Sold','Year Remod/Add']].loc[2180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f02f643d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "years_until_remod      -1\n",
       "Year Remod/Add       2001\n",
       "Year Built           2002\n",
       "Name: 850, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['years_until_remod','Year Remod/Add','Year Built']].loc[850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa5cd2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "years_before_sale      -1\n",
       "Yr Sold              2007\n",
       "Year Built           2008\n",
       "Name: 2180, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['years_before_sale','Yr Sold','Year Built']].loc[2180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9295d7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6010f",
   "metadata": {},
   "source": [
    "from our observations so far, there has to be some discrepancies which speaks to possibly human error when computing the data.\n",
    "\n",
    "for instance, it is impossible for a house to be built in 2008 and sold in 2007. it is also unlikely for a house to be built in 2002 and remodelled in 2001.\n",
    "\n",
    "so we are going o drop all the rows with negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46bd2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with negative values for both of these new features\n",
    "df = df.drop([850, 1702,2180,2181], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d557e086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "081add25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which columns need to be dropped for other reasons?\n",
    "\n",
    "## Drop columns that aren't useful for ML\n",
    "df = df.drop([\"PID\", \"Order\"], axis=1)\n",
    "\n",
    "## Drop columns that leak info about the final sale\n",
    "df = df.drop([\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20432f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db8da4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MS SubClass', 'MS Zoning', 'Lot Area', 'Street', 'Lot Shape',\n",
       "       'Land Contour', 'Utilities', 'Lot Config', 'Land Slope', 'Neighborhood',\n",
       "       'Condition 1', 'Condition 2', 'Bldg Type', 'House Style',\n",
       "       'Overall Qual', 'Overall Cond', 'Year Built', 'Year Remod/Add',\n",
       "       'Roof Style', 'Roof Matl', 'Exterior 1st', 'Exterior 2nd',\n",
       "       'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation',\n",
       "       'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF',\n",
       "       'Heating', 'Heating QC', 'Central Air', '1st Flr SF', '2nd Flr SF',\n",
       "       'Low Qual Fin SF', 'Gr Liv Area', 'Bsmt Full Bath', 'Bsmt Half Bath',\n",
       "       'Full Bath', 'Half Bath', 'Bedroom AbvGr', 'Kitchen AbvGr',\n",
       "       'Kitchen Qual', 'TotRms AbvGrd', 'Functional', 'Fireplaces',\n",
       "       'Garage Cars', 'Garage Area', 'Paved Drive', 'Wood Deck SF',\n",
       "       'Open Porch SF', 'Enclosed Porch', '3Ssn Porch', 'Screen Porch',\n",
       "       'Pool Area', 'Misc Val', 'SalePrice', 'years_until_remod',\n",
       "       'years_since_remod', 'years_before_sale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab179e6c",
   "metadata": {},
   "source": [
    "now let's update our `transform_features()` function with what we have done so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7b4b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55284.62277814025"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a function named transform_features() that, for now, just returns the train data frame.\n",
    "def transform_features(df):\n",
    "    percent_df=df.isnull().sum() * 100/len(df)\n",
    "#drop columns with missing values greater than 5%\n",
    "    missing_val_greater_5=percent_df[(percent_df>5)]\n",
    "    df=df.drop(missing_val_greater_5.index,axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    text_cols_count=text_cols.isnull().sum()\n",
    "## Filter Series to columns containing *any* missing values\n",
    "    missing_text_cols = text_cols_count[text_cols_count > 0]\n",
    "    df = df.drop(missing_text_cols.index, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "   \n",
    "    df['years_until_remod']= df['Year Remod/Add'] - df['Year Built']\n",
    "    df['years_since_remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    df['years_before_sale']= df['Yr Sold'] - df['Year Built']\n",
    "    ## Drop rows with negative values for both of these new features\n",
    "    df = df.drop([850, 1702,2180,2181], axis=0)\n",
    "## Drop columns that leak info about the final sale\n",
    "    df = df.drop([\"PID\", \"Order\",\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1) \n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
    "\n",
    "def train_and_test(df):  \n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    \n",
    "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
    "    ## and return only those columns as a data frame.\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    ## You can use `pd.Series.drop()` to drop a value.\n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae3184",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Now that we have cleaned and transformed a lot of the features in the data set, it's time to move on to feature selection for numerical features.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Generate a correlation heatmap matrix of the numerical features in the training data set.\n",
    "  * Which features correlate strongly with our target column, `SalePrice`?\n",
    "  * Calculate the correlation coefficients for the columns that seem to correlate well with `SalePrice`. Because we have a pipeline in place, it's easy to try different features and see which features result in a better cross validation score.\n",
    "\n",
    "* Which columns in the data frame should be converted to the categorical data type? All of the columns that can be categorized as nominal variables are candidates for being converted to categorical. Here are some other things you should think about:\n",
    "  * If a categorical column has hundreds of unique values (or categories), should you keep it? When you dummy code this column, hundreds of columns will need to be added back to the data frame.\n",
    "  * Which categorical columns have a few unique values but more than 95% of the values in the column belong to a specific category? This would be similar to a low variance numerical feature (no variability in the data for the model to capture).\n",
    "\n",
    "* Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don't have any semantic meaning)?\n",
    "* What are some ways we can explore which categorical columns \"correlate\" well with `SalePrice`?\n",
    "* Update the logic for the `select_features()` function. This function should take in the new, modified train and test data frames that were returned from `transform_features()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "136890e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BsmtFin SF 2         0.006156\n",
      "Misc Val             0.019264\n",
      "3Ssn Porch           0.032279\n",
      "Bsmt Half Bath       0.035852\n",
      "Low Qual Fin SF      0.037620\n",
      "Pool Area            0.068445\n",
      "MS SubClass          0.085056\n",
      "Overall Cond         0.101498\n",
      "Screen Porch         0.112310\n",
      "Kitchen AbvGr        0.119743\n",
      "Enclosed Porch       0.128656\n",
      "Bedroom AbvGr        0.143902\n",
      "Bsmt Unf SF          0.182862\n",
      "years_until_remod    0.240017\n",
      "Lot Area             0.267517\n",
      "2nd Flr SF           0.269707\n",
      "Bsmt Full Bath       0.276214\n",
      "Half Bath            0.284974\n",
      "Open Porch SF        0.316277\n",
      "Wood Deck SF         0.328158\n",
      "BsmtFin SF 1         0.439365\n",
      "Fireplaces           0.474994\n",
      "TotRms AbvGrd        0.498614\n",
      "Mas Vnr Area         0.507010\n",
      "Year Remod/Add       0.532996\n",
      "years_since_remod    0.534972\n",
      "Full Bath            0.546108\n",
      "Year Built           0.558499\n",
      "years_before_sale    0.558984\n",
      "1st Flr SF           0.635183\n",
      "Garage Area          0.641414\n",
      "Total Bsmt SF        0.644023\n",
      "Garage Cars          0.648351\n",
      "Gr Liv Area          0.717617\n",
      "Overall Qual         0.801212\n",
      "SalePrice            1.000000\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Generate a correlation heatmap matrix of the numerical features in the training data set.\n",
    "numerical_df=transform_df.select_dtypes(include=['integer', 'float'])\n",
    "corr_series = numerical_df.corr()  #check the correlation\n",
    "sorted_corrs = np.abs(corr_series['SalePrice']).sort_values() #sort the dataset according to the target column 'sale price'\n",
    "print(sorted_corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df71b7a",
   "metadata": {},
   "source": [
    "from what we can see so far, `Gr Liv Area` and `Overall Qual` have the stronges correlation with `SalePrice`. For now, let's keep only the features that have a correlation of 0.3 or higher. This cutoff is a bit arbitrary and, in general, it's a good idea to experiment with this cutoff. For example, you can train and test models using different cutoffs and see where your model stops improving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8a3eb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open Porch SF        0.316277\n",
       "Wood Deck SF         0.328158\n",
       "BsmtFin SF 1         0.439365\n",
       "Fireplaces           0.474994\n",
       "TotRms AbvGrd        0.498614\n",
       "Mas Vnr Area         0.507010\n",
       "Year Remod/Add       0.532996\n",
       "years_since_remod    0.534972\n",
       "Full Bath            0.546108\n",
       "Year Built           0.558499\n",
       "years_before_sale    0.558984\n",
       "1st Flr SF           0.635183\n",
       "Garage Area          0.641414\n",
       "Total Bsmt SF        0.644023\n",
       "Garage Cars          0.648351\n",
       "Gr Liv Area          0.717617\n",
       "Overall Qual         0.801212\n",
       "SalePrice            1.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select only features with atleast 0.3 correlation\n",
    "sorted_corrs_more_3=sorted_corrs[sorted_corrs>0.3]\n",
    "sorted_corrs_more_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2049361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns with less than 0.4 correlation with SalePrice\n",
    "transform_df = transform_df.drop(sorted_corrs[sorted_corrs<0.3].index, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfa350",
   "metadata": {},
   "source": [
    "Which columns in the data frame should be converted to the categorical data type? \n",
    "\n",
    "All of the columns that can be categorized as nominal variables are candidates for being converted to categorical. Here are some other things you should think about:\n",
    "\n",
    "* If a categorical column has hundreds of unique values (or categories), should you keep it? When you dummy code this column, hundreds of columns will need to be added back to the data frame.\n",
    "* Which categorical columns have a few unique values but more than 95% of the values in the column belong to a specific category? This would be similar to a low variance numerical feature (no variability in the data for the model to capture).\n",
    "\n",
    "to answer this, let's take a look at some of the unique values in the `text_cols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8d58ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select text feature columns\n",
    "text_cols=transform_df.select_dtypes(include=['object'])\n",
    "text_unique_count=text_cols.nunique().sort_values()\n",
    "drop_text_cols=text_unique_count[text_unique_count>10]\n",
    "#drop text features woth more than 10 unique values\n",
    "transform_df=transform_df.drop(drop_text_cols.index,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e945df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select just the remaining text columns and convert to categorical\n",
    "text_cols = transform_df.select_dtypes(include=['object'])\n",
    "for col in text_cols:\n",
    "    transform_df[col] = transform_df[col].astype('category')\n",
    "    \n",
    "## Create dummy columns and add back to the dataframe!\n",
    "transform_df = pd.concat([\n",
    "    transform_df, \n",
    "    pd.get_dummies(transform_df.select_dtypes(include=['category']))\n",
    "], axis=1).drop(text_cols,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2f8bc",
   "metadata": {},
   "source": [
    "at this point, update your `select features()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2753eb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33468.08890531518"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a function named transform_features() that, for now, just returns the train data frame.\n",
    "def transform_features(df):\n",
    "    percent_df=df.isnull().sum() * 100/len(df)\n",
    "#drop columns with missing values greater than 5%\n",
    "    missing_val_greater_5=percent_df[(percent_df>5)]\n",
    "    df=df.drop(missing_val_greater_5.index,axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    text_cols_count=text_cols.isnull().sum()\n",
    "## Filter Series to columns containing *any* missing values\n",
    "    missing_text_cols = text_cols_count[text_cols_count > 0]\n",
    "    df = df.drop(missing_text_cols.index, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "   \n",
    "    df['years_until_remod']= df['Year Remod/Add'] - df['Year Built']\n",
    "    df['years_since_remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    df['years_before_sale']= df['Yr Sold'] - df['Year Built']\n",
    "    ## Drop rows with negative values for both of these new features\n",
    "    df = df.drop([850, 1702,2180,2181], axis=0)\n",
    "## Drop columns that leak info about the final sale\n",
    "    df = df.drop([\"PID\", \"Order\",\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1) \n",
    "    return df\n",
    "\n",
    "#i experimemnted with the coeff threshold before i came up with 0.4\n",
    "# i tried 0.3 but the rmse is high, and i tried 0.5,0.6...it looks like\n",
    "#0.4 has better results\n",
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    #select text feature columns\n",
    "    text_cols=df.select_dtypes(include=['object'])\n",
    "    text_unique_count=text_cols.nunique().sort_values()\n",
    "    drop_text_cols=text_unique_count[text_unique_count>10]\n",
    "    #drop text features woth more than 10 unique values\n",
    "    df=df.drop(drop_text_cols.index,axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1).drop(text_cols,axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def train_and_test(df):  \n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    \n",
    "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
    "    ## and return only those columns as a data frame.\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    ## You can use `pd.Series.drop()` to drop a value.\n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01851ae8",
   "metadata": {},
   "source": [
    "## Train And Test\n",
    "Now for the final part of the pipeline, training and testing. When iterating on different features, using simple validation is a good idea. Let's add a parameter named k that controls the type of cross validation that occurs.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* The optional `k` parameter should accept integer values, with a default value of `0`.\n",
    "* When `k` equals `0`, perform holdout validation (what we already implemented):\n",
    "  * Select the first `1460` rows and assign to `train`.\n",
    "  * Select the remaining rows and assign to `test`.\n",
    "  * Train on `train` and test on `test`.\n",
    "  * Compute the RMSE and return.\n",
    "* When `k` equals `1`, perform simple cross validation:\n",
    "  * Shuffle the ordering of the rows in the data frame.\n",
    "  * Select the first `1460` rows and assign to `fold_one`.\n",
    "  * Select the remaining rows and assign to `fold_two`.\n",
    "  * Train on `fold_one` and test on `fold_two`.\n",
    "  * Train on `fold_two` and test on `fold_one`.\n",
    "  * Compute the average RMSE and return.\n",
    "* When `k` is greater than `0`, implement k-fold cross validation using `k` folds:\n",
    "  * Perform k-fold cross validation using k folds.\n",
    "  * Calculate the average RMSE value and return this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "930b1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36550.23773803087, 26985909.036646564, 9427487.033298733, 272324908.0519385]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77193713.58990546"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a function named transform_features() that, for now, just returns the train data frame.\n",
    "def transform_features(df):\n",
    "    percent_df=df.isnull().sum() * 100/len(df)\n",
    "#drop columns with missing values greater than 5%\n",
    "    missing_val_greater_5=percent_df[(percent_df>5)]\n",
    "    df=df.drop(missing_val_greater_5.index,axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    text_cols_count=text_cols.isnull().sum()\n",
    "## Filter Series to columns containing *any* missing values\n",
    "    missing_text_cols = text_cols_count[text_cols_count > 0]\n",
    "    df = df.drop(missing_text_cols.index, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "   \n",
    "    df['years_until_remod']= df['Year Remod/Add'] - df['Year Built']\n",
    "    df['years_since_remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    df['years_before_sale']= df['Yr Sold'] - df['Year Built']\n",
    "    ## Drop rows with negative values for both of these new features\n",
    "    df = df.drop([850, 1702,2180,2181], axis=0)\n",
    "## Drop columns that leak info about the final sale\n",
    "    df = df.drop([\"PID\", \"Order\",\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1) \n",
    "    return df\n",
    "\n",
    "#i experimemnted with the coeff threshold before i came up with 0.4\n",
    "# i tried 0.3 but the rmse is high, and i tried 0.5,0.6...it looks like\n",
    "#0.4 has better results\n",
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    #select text feature columns\n",
    "    text_cols=df.select_dtypes(include=['object'])\n",
    "    text_unique_count=text_cols.nunique().sort_values()\n",
    "    drop_text_cols=text_unique_count[text_unique_count>10]\n",
    "    #drop text features woth more than 10 unique values\n",
    "    df=df.drop(drop_text_cols.index,axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1).drop(text_cols,axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_and_test(df,k=0):\n",
    "    ## You can use `pd.DataFrame.select_dtypes()` to specify column types\n",
    "    ## and return only those columns as a data frame.\n",
    "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
    "    lr  = LinearRegression()\n",
    "    features = numeric_df.columns.drop(\"SalePrice\")\n",
    "    from sklearn.model_selection import KFold\n",
    "    if k==0:\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "        ## You can use `pd.Series.drop()` to drop a value.\n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions = lr.predict(test[features])\n",
    "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "   \n",
    "    if k==1:\n",
    "         #Use the np.random.permutation() function to return a NumPy array of shuffled index values\n",
    "        df = df.loc[np.random.permutation(len(df))]\n",
    "        fold_one = df[:1460]\n",
    "        fold_two = df[1460:]\n",
    "        lr.fit(fold_one[features], fold_one[\"SalePrice\"])\n",
    "        predictions1 = lr.predict(fold_two[features])\n",
    "        lr.fit(fold_two[features], fold_two[\"SalePrice\"])\n",
    "        predictions2 = lr.predict(fold_one[features])\n",
    "        mse1 = mean_squared_error(fold_two[\"SalePrice\"], prediction1)\n",
    "        mse2 = mean_squared_error(fold_one[\"SalePrice\"], prediction2)\n",
    "        rmse1 = np.sqrt(mse1)\n",
    "        rmse2 = np.sqrt(mse2)\n",
    "        avg_rmse = np.mean([rmse1, rmse2])\n",
    "        \n",
    "        return avg_rmse\n",
    "    else:\n",
    "        kf = KFold(n_splits=k, shuffle=True)\n",
    "        rmse_values = []\n",
    "        for train_index, test_index, in kf.split(df):\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            lr.fit(train[features], train[\"SalePrice\"])\n",
    "            predictions = lr.predict(test[features])\n",
    "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "        print(rmse_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        return avg_rmse        \n",
    "\n",
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df, k=4)\n",
    "\n",
    "rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be15aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0359b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024124e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634be8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b4206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
