{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd2df06",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the Guided Project for the Logistic Regression Modeling in Python course! We have learned a lot about logistic regression and classification in the past four lessons, and it's about time that we use this knowledge on a real-world dataset.\n",
    "\n",
    "As with the linear regression guided project, we'll also be looking at a real-life dataset: the [Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) from the UCI Machine Learning Repository. This dataset comes from the famous Cleveland Clinic Foundation, which recorded information on various patient characteristics, including age and chest pain, to try to classify the presence of heart disease in an individual. This a prime example of how machine learning can help solve problems that have a real impact on people's lives.\n",
    "\n",
    "We'll practice going through the machine learning pipeline, starting from examining the dataset itself to creating a polished classification model. Classification problems are much more common than regression problems, so it'll be good to get some practice.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Instructions\n",
    "You may work on this guided project in a personal Jupyter notebook or Python coding environment of your choice, but feel free to use the Dataquest interface. Going through a guided project on your own local machine makes it more convenient to share and iterate on. The heart_disease.csv file is currently loaded into the interface, and you can download it here to have on your own machine.\n",
    "\n",
    "- Load in the pandas library with pd as the alias.\n",
    "- To get started, let's load the heart disease dataset in so that we can start examining it:\n",
    "- Use the `read_csv()` function to read in the data and name the data heart\n",
    "\n",
    "Note: we have partially cleaned the dataset so that we will perform binary classification. The original dataset has multiple classes, so you may download it from the site to attempt the guided project on multiple classes instead. For this guided project, we will focus on the binary classification case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f87da4",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "\n",
    "Before we build any model, we should explore the dataset and perform any adjustments we might need before actually fitting the model. This may include converting categorical variables into dummy variables or centering and scaling variables. We'll also want to check for predictors that are distributed differently based on the outcome, since they could be informative for classification.\n",
    "\n",
    "Take some time to explore the heart dataset. We'll want to ultimately pick out some predictors to potentially use in a linear model.\n",
    "\n",
    "## Instructions\n",
    "- What are the columns that are present in the dataset?\n",
    "- Consult the official page on the dataset to read about it a bit more.\n",
    "- What are the data types for each of the columns? Are any of them worth transforming or converting into dummy variables?\n",
    "- Check the relationships between the potential predictor variables with the outcome via plots (i.e., histograms). See if stratifying by heart disease shows a meaningful difference in the distribution of the predictors:\n",
    "- The `boxplot()` and `hist()` methods may come in handy here.\n",
    "- Select a set of the predictors to use in your predictor model.\n",
    "- Summarize these findings in a short paragraph before moving on to the next screen.\n",
    "- Why did you choose to include your predictors in the model?\n",
    "- Were there data-specific reasons? Domain knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81644c55",
   "metadata": {},
   "source": [
    "## Dividing the Data\n",
    "\n",
    "Now that we have some predictors, we need to set aside some data to act as a final assessment for our model. We'll need the following:\n",
    "\n",
    "A training set that will be used to estimate the regression coefficients\n",
    "A test set that will be used to assess the predictive ability of the model\n",
    "The model will be fit to the training set, and predictive ability will be assessed on the test set. We'll need to make sure that both sets contain both cases and non-cases.\n",
    "\n",
    "Let's take the time to divide up the data properly.\n",
    "\n",
    "## Instructions\n",
    "- Decide what percentage of the `heart` dataset will be used for the training and test datasets. The remaining proportion will be used for the test set.\n",
    "- Import the `train_test_split()` function from the model_selection submodule in sklearn.\n",
    "- Using this proportion, divide up the `heart` data into a training set and a test set:\n",
    "- Make sure to set a random seed to make your results reproducible.\n",
    "Check that both the training and test datasets have cases and non-cases. If not, then select a new seed until this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cffb70",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "With our `heart` dataset divided up, let's build the classification model and do some initial assessments. These are some guiding questions that you should think about:\n",
    "\n",
    "- What is the overall training accuracy? Sensitivity and specificity?\n",
    "- Does the model perform better on cases or non-cases? Or does it perform equally well?\n",
    "\n",
    "These training metrics are overly optimistic estimations of how the model performs, so we should expect slightly worse metrics if the model is general enough. If these metrics are too high, it might be a sign that our model is starting to overfit.\n",
    "\n",
    "## Instructions\n",
    "- Construct the logistic regression model using only the training set.\n",
    "- Calculate the accuracy, sensitivity and specificity of the model.\n",
    "- Write some notes about what you observe from these measures of model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba517b",
   "metadata": {},
   "source": [
    "## Interpreting the Model Coefficients\n",
    "\n",
    "Now that we've created our model, let's look at the coefficients to see if they make sense, given the problem. Recall that the logistic regression relates the binary outcome to the linear combination of predictors via the link function:\n",
    "\n",
    "$log(\\frac{EY}{1−EY})=β_0+β_1X $\n",
    "\n",
    "The predictors affect the outcome on the log-odds scale. The non-intercept coefficients represent the log-odds ratio for a unit increase in a predictor:\n",
    "\n",
    "$log(\\frac{O_1}{O_0})=β_1$\n",
    "\n",
    ". . . where $O_0$ represents the odds ratio when the predictor is `0`, and $O_1$ represents the odds ratio when the predictor is `1`. However, we're usually interested in examining these effects on the odds scale, so we take e to both sides to get the following:\n",
    "\n",
    "$O1=e^{β_1}O_0$\n",
    "\n",
    "Let's see what our chosen predictors suggest about their relationship with heart disease.\n",
    "\n",
    "## Instructions\n",
    "- Examine the coefficients of your logistic regression model on both the `log-odds` and odds scales.\n",
    "- Make some notes on what the coefficients suggest about the effects of the predictors. Do these coefficients seem to make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b6915",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n",
    "\n",
    "Finally, we can assess the predictive ability of our logistic regression model.\n",
    "\n",
    "## Instructions\n",
    "- Use the model to calculate the test predictions.\n",
    "- Calculate the accuracy, sensitivity, and specificity of these predictions.\n",
    "- How does this value compare to what was calculated from the training set?\n",
    "- Write down some conclusions on what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e2c3d",
   "metadata": {},
   "source": [
    "## Drawing Conclusions\n",
    "\n",
    "We hope that the process of going from dataset to model or set of models is starting to feel more natural. As you learn more machine learning, continue to refine your own personal process. Take some time to review all of the notes that you've made during the predictive modeling process.\n",
    "\n",
    "## Instructions\n",
    "Answer the following questions in your write-up:\n",
    "\n",
    "- Does the model make sense when considering its interpretation? Does it seem to match up with what you might expect?\n",
    "- Does the model seem to predict the cases or non-cases better than the other? Why might this be the case, based on your model?\n",
    "- How would you interpret the accuracy for the model? Does this accuracy seem acceptable for use in an actual clinical setting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6134d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fbef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699ff86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6099878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea085cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647174a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00238c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2983555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be5e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858f110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa9253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9fc62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7700f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d127e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
